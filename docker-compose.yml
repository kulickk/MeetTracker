services:
  postgres_db:
    image: postgres:17.2
    container_name: "postgres_db"
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=Nikita8321
      - POSTGRES_DB=postgres
    ports:
      - "5432:5432"
    networks:
      - back-tier
    expose:
      - 5432
    volumes:
      - .postgres:/var/lib/postgres/data
  meet-tracker:
    build:
      context: .
    ports:
      - "8000:8000"
    networks:
      - back-tier
    restart: always
    depends_on:
      - postgres_db
    env_file:
      - .env
    volumes:
      - ./:/app
  whisper:
    build:
      context: ./src
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    ports:
      - "8081:8081"
    networks:
      - back-tier
    restart: always
    depends_on:
      - postgres_db
    env_file:
      - .env
    volumes:
      - ./src:/app/src
  # ollama:
  #   volumes:
  #     - ./ollama/ollama:/root/.ollama
  #   container_name: ollama
  #   pull_policy: always
  #   tty: true
  #   restart: unless-stopped
  #   image: ollama/ollama:latest
  #   ports:
  #     - 7869:7869
  #     - 11434:11434
  #   networks:
  #     - back-tier
  #   environment:
  #     - OLLAMA_KEEP_ALIVE=24h
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [ gpu ]
  #   command: ollama pull kulick/t-lite:latest && ollama serve
    

      # ollama:
      #   volumes:
      #     - ./ollama/ollama:/root/.ollama
      #   container_name: ollama
      #   pull_policy: always
      #   tty: true
      #   restart: unless-stopped
      #   image: ollama/ollama:latest
      #   ports:
      #     - 7869:7869
      #     - 11434:11434
      #   environment:
      #     - OLLAMA_KEEP_ALIVE=24h
      #   networks:
      #     - ollama-docker
      #   deploy:
      #     resources:
      #       reservations:
      #         devices:
      #           - driver: nvidia
      #             count: all
      #             capabilities: [gpu]
      # ollama-webui:
      #   image: ghcr.io/open-webui/open-webui:main
      #   container_name: ollama-webui
      #   volumes:
      #     - ./ollama/ollama-webui:/app/backend/data
      #   depends_on:
      #     - ollama
      #   ports:
      #     - 8085:8080
      #   environment: # https://docs.openwebui.com/getting-started/env-configuration#default_models
      #     - OLLAMA_BASE_URLS=http://host.docker.internal:7869 #comma separated ollama hosts
      #     - ENV=dev
      #     - WEBUI_AUTH=False
      #     - WEBUI_NAME=valiantlynx AI
      #     - WEBUI_URL=http://localhost:8080
      #     - WEBUI_SECRET_KEY=t0p-s3cr3t
      #   extra_hosts:
      #     - host.docker.internal:host-gateway
      #   restart: unless-stopped
      #   networks:
      #     - ollama-docker
networks:
  back-tier: {}